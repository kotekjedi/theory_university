\documentclass[11pt]{book}
\input{preamble.tex}

\title{Дебильник}
\date{\today}
\author{Вячеслав Тамарин}

\begin{document}
\chapter{Дебильник}
\section{Многомерное нормальное распределение}
\begin{definition}
	\selectedFont{Стандартный гауссовский вектор} --- случайный $n$-мерный вектор $ Z = (Z_1, Z_2, \ldots Z_{n})$, координаты которого независимы и имеют распределение $ \N(0, 1)$.
\end{definition}

\begin{definition}[]
	\selectedFont{Гауссовский вектор} (\selectedFont{Нормальный вектор}) --- вектор, для которого существует матрица $ {\bf A} \in \R^{n \times m} $, стандартный гауссовский вектор $ Z \in \R^{m}$, и вектор $ b \in \R^{n}$ такие, что $ X = {\bf A}Z + b $.
\end{definition}

\begin{definition}[]
	\selectedFont{Распределение нормального вектора} $ X \in \R^{n}$ --- $ \N(\mu, {\bf \Sigma}) $ или $ \N_n(\mu, {\bf \Sigma}) $, где $ \mu = \E X$ и $ {\bf \Sigma} = \cov(X) $.
\end{definition}

\begin{definition}[]
	\selectedFont{Распределение хи-квадрат} с $ n$ степенями свободы --- распределение $ \chi^2(n) $ величины $ \chi^2 = Z_1^2 + Z_2^2 + \ldots + Z_{n}^2 $, где $ Z_{1}, Z_2, \ldots Z_{n}$ --- независимы $ \N(0, 1)$ величины. 
\end{definition}

\begin{definition}[]
	\selectedFont{Распределение Стьюдента} с $n$ степенями свободы --- распределение $T(n)$ величины $\frac{\sqrt{n}X}{\sqrt{Y}}$, где $X \sim \N(0, 1)$, $Y \sim \chi^2(n)$ и независимы.
\end{definition}

\begin{definition}[]
	\selectedFont{Распределение Фишера} со степенями свободы $n$ и $m$ --- распределение $F(n, m)$ величины $\frac{X / n}{Y / m}$, где $X \sim \chi^2(n)$, $Y \sim \chi^2(m)$ и независимы.
\end{definition}

\section{Условное матожидание}
\begin{definition}[]
	\selectedFont{Условное матожидание} $\E (Y \mid X)$ случайной величины $Y$ при условии случайной величины $X$ --- такая измеримая функция $g_0$ величины $X$, при которой $\E(Y - g(X))^2$ минимально для всех измеримых функций $g$.
\end{definition}
Условное матожидание --- ортогональная проекция $Y$ на линейное пространство всех измеримых функций $X$. То есть УМО --- единственная измеримая функция, которая удовлетворяет условию ортогональности:
 \[
\forall g \colon\E(Y - \E(Y \mid X)) g(X) = 0
.\] 

\section{Статистическая модель, выборка}
\begin{definition}[]
	\selectedFont{Статистическая модель} --- множество распределений $\PP$, которое, по нашему мнению, адекватно приближает $\P_D$. 
\end{definition}
\begin{definition}[]
	\selectedFont{Данные} $d$ --- реализация случайного элемента $D$, имеющего распределение $\P_D$.
\end{definition}
Статистические модели делят на:
\begin{itemize}
	\item \selectedFont{параметрические}, если $\PP = \{\P_0 \mid \theta \in \Theta \subset \R^{k}\}$.

		Пример: $\PP = \{\N(\mu, \sigma^2) \mid \mu \in \R, \sigma^2 \ge 0\}$.
	\item \selectedFont{непараметрические}, если $\PP = \{\P_0 \mid \theta \in  \Theta \subset V\}$, где $V$ не обязательно конечномерное.

		Пример: $\PP = \{\P^{\otimes n} \mid \int_{\mathfrak{X}}^{} x \P(dx) = 0 \}$
	\item \selectedFont{семипараметрические}, если $\PP = \{\P_0 \mid \theta \in \Theta \subset \R^{k} \times V\}$.

		Пример: линейная регрессия $Y = X \beta + \varepsilon$, $\beta \in \R^{k}$, $\E \varepsilon = 0$, $\D \varepsilon = \sigma^2$.
\end{itemize}
Если $D = [X_1, \ldots X_n]$ и $X_i$ независимы и имеют одинаковое распределение $\P_{X}$, $D$ называется \selectedFont{выборкой объема} $n$ и обозначается $X_{[n]}$, $\P_{X}$ --- генеральная совокупность.
В этом случае модель приобретает вид
$ \PP = \{\P^{\otimes n} \mid \P \in \PP_X\} $, где $\PP_X$ --- модель для $\P_X$.

\section{Формула Байеса, априорное, апостериорное распределение}
\begin{itemize}
	\item \selectedFont{Априорное распределение} --- наше ощущение относительно значения параметра до проведения эксперимента.
	\item \selectedFont{Апостериорное распределение} --- ощущение после получения данных эксперимента.
\end{itemize}
\begin{definition}[Формула Байеса]
	Здесь $p$ --- вероятность, $d$ --- данные, $\theta$ --- параметры.
	\[
	p(\theta \mid d) = \frac{p(d \mid \theta) \cdot p(\theta)}{p(d)}
	.\] 
	\begin{itemize}
		\item $p(\theta \mid d)$ --- апостериорное распределение,
		\item $p(d \mid \theta)$ --- правдоподобие,
		\item $p(\theta)$ --- априорное распределение,
		\item $p(d)$ --- вероятность данных.
	\end{itemize}
\end{definition}

\section{Расстояние Кульбака-Лейблера, энтропия}
Пусть мы принимаем случайные символы $x_1, \ldots x_{k}$, вероятность появления $x_{i}$ равна $p_i$, записываем с помощью битовой строки длины $l_i$. Тогда средняя длина символа равна 
\[
l = \sum_{i=1}^{k} p_i \cdot l_i
.\] 
Чтобы минимизировать $l$, необходимо подобрать следующие $l_i = - \log_2{p_i}$. И тогда средняя длина будет равна $H(x) \coloneqq - \sum_{i=1}^{k} p_i \cdot  \log_2{p_i}$, эта величина называется \selectedFont{двоичной энтропией сообщения}. Аналогично можно брать любой другой логарифм, мы будем использовать натуральный.

Для непрерывной величины можно завести \selectedFont{дифференциальную энтропию}:
\[
H(X) = - \int_{}^{} p(x) \log{p(x)} dx 
.\] 
Пусть случайная величина $X$ имеет функцию вероятности $p$, но мы кодируем символы, как-будто она имеет функцию вероятности $q$. Тогда средняя длина сообщения будет равна $- \sum_{i=1}^{k} p_i \cdot \log{q_i}$, эта величина называется \selectedFont{кросс-энтропией} $H(p \mid q)$ распределений $p$ и $q$.

$H(p \mid q)$ всегда будет больше $H(p)$, так как $H(p)$ минимально.
\begin{definition}[]
	Величина потери информации из-за использования $q$ вместо $p$ называется \selectedFont{расстоянием Кульбака-Лейблера} между $p$ и $q$:
	\[
	D_{KL} (p, q) = H(p \mid q) - H(p) = - \sum_{i=1}^{k} p_i \cdot \log{\frac{q_i}{p_i}}
	.\] 
\end{definition}
Для непрерывных величин все обобщается следующим образом 
\[
D_{KL} = - \int p_i \cdot \log{\frac{q_{i}}{p_i}}
.\] 

\section{Статистика...}
\subsection{Статистика}
Параметр или характеристика распределения --- функционал от этого распределения.
\begin{definition}[]
	\selectedFont{Статистика} ---  функция $\theta^*$ от данных $d$.
\end{definition}

Пусть модель $\PP_{[n]} = \{\P ^{\otimes n} \mid \P \in \PP\}$, искомая характеристика $\theta\colon \PP \to \R^{k}$.

\subsection{Несмещенность}
Чему равна оценка как случайная величина в среднем, если она равна характеристике?

\begin{definition}[]
	Оценка $\Theta^*$ называется
	\begin{itemize}
		\item \selectedFont{несмещенной}, если $\forall \P \in \PP\colon \E \theta^*(X_{[n]}) = \theta(\P)$, где $X_{[n]} \sim \P^{\otimes n}$,
		\item \selectedFont{асимптотически несмещенной}, если $\forall \P \in \PP\colon \E \theta^*(X_{[n]}) \to \theta(\P)$.
	\end{itemize}

	\selectedFont{Смещение} --- величина $b(\theta^*) = \E(\theta^*(X_{[n]})) - \theta(\P)$.

	\selectedFont{Среднеквадратичная ошибка} --- величина $\MSE(\theta^*) = \E \left( \theta^*(X_{[n]}) - \theta(\P) \right) ^2$.
\end{definition}
В общем случае 
\[
\MSE(\theta^*) = \D \theta^*(X_{[n]} + b^2(\theta^*)
.\] 
\begin{itemize}
	\item Выборочное среднее как оценка матожидания --- несмещенная оценка,
	\item Выборочная дисперсия как оценка дисперсии --- асимптотически несмещенная,
	\item Исправленная выборочная дисперсия как оценка дисперсии --- несмещенная оценка.
\end{itemize}

\subsection{Состоятельность}
\begin{definition}[]
	Оценка $\theta^*$ называется
	\begin{itemize}
		\item \selectedFont{состоятельной}, если $\forall \P \in \PP\colon \theta^*(X_{[n]}) \xrightarrow{\mathbb{P}} \theta(\P)$, где $X_{[n]} \sim \P^{\otimes n}$,
		\item \selectedFont{сильно состоятельной}, если $\theta^*(X_{[n]}) \xrightarrow{\text{п. н.}} \theta(\P)$.
	\end{itemize}
\end{definition}

\subsection{Асимптотическая нормальность}
\begin{definition}[]
	Оценка $\theta^*$ называется \selectedFont{асимптотически нормальной} с коэффициентом рассеивания (или просто дисперсией) $\sigma^2\bigl(\theta(\P)\bigr > 0$, если 
		\[
		\sqrt{n} \left( \theta^*(X_{[n]}) - \theta(\P) \right) \xrightarrow{d} \eta \sim \N(0, \sigma^2(\theta^*(\P)))
		.\] 
	В многомерном случае рассматривается ковариационная матрица вместо дисперсии.
\end{definition}
\begin{itemize}
	\item Выборочная дисперсия и второй момент --- асимптотически нормальная оценка.
	\item Из асимптотической нормальности следует состоятельность.
\end{itemize}

\subsection{Эффективность}
Рассмотрим класс оценок $K = \{\hat{\theta}\}$ параметра $\theta$.
\begin{definition}[]
	Оценка $\theta^* \in K$ называется \selectedFont{эффективной в классе} $K$, если \it{для любой другой оценки} $\hat{\theta} \in  K$ и \it{для любого исследуемого параметра} $\theta \in \Theta$ выполняется
	\[
	\MSE_{\theta}(\theta^*) \le \MSE_{\theta}(\hat{\theta})
	.\] 
\end{definition}
Класс несмещенных оценок
 \[
K_0 = \{\hat{\theta} \mid \E {\hat{\theta}} = \theta, \forall \theta \in \Theta\}
.\] 
\begin{definition}[]
		\selectedFont{Эффективная оценка} $\theta^*$, если эффективна в классе $K_0$.
\end{definition}
\begin{definition}[]
		\selectedFont{Асимптотически эффективной в классе} $K$, если для любой оценки $\hat{\theta} \in K$ и для любого $\theta \in  \Theta$ выполняется
			\[
			\overline{\lim_{n \to \infty}} \frac{\MSE(\theta^*)}{\MSE(\hat{\theta}}
			.\] 
\end{definition}

\subsection{Робастность}
\begin{definition}[]
	\selectedFont{Робастность} --- свойство оценки быть устойчивой к хвостам распределения.
\end{definition}
	Пусть $F$ --- распределение,  $\{G_{n}\}$ --- последовательность распределений, что
	\[
	\lvert F - G_{n}\rvert \coloneqq \underset{x}{\sup} \lvert F(x) - G_n(x) \rvert \to 0
	.\] 
\begin{definition}[]
	Характеристика $\theta$ обладает \selectedFont{качественной робастностью}, если $\theta(G_{n}) \to \theta(F)$
\end{definition}
Пусть также $\delta_x$ --- вырожденное распределение в точке $x$.
\begin{definition}[]
	\selectedFont{Загрязненное распределение} --- смесь $F_{x, \varepsilon} = (1-\varepsilon) F + \varepsilon \delta_x$.	
\end{definition}
\begin{definition}[]
	\selectedFont{Функция влияния} характеристики $\theta$ --- величина $$IF(x) = \lim_{\varepsilon \to 0+} \frac{\theta(F_{x, \varepsilon}) - \theta(F)}{\varepsilon}.$$
\end{definition}
\begin{definition}[]
	Характеристика $\theta$ называется \selectedFont{$B$-робастной} или \selectedFont{инфинитезимально робастной}, если $IF(x)$ ограничена.
\end{definition}
\begin{definition}[]
	\selectedFont{Асимптотическая толерантность} характеристики $\theta$ ---
	\[
	\tau = \inf \bigl\{ \varepsilon \mid \underset{x}{\sup}\lvert \theta(F_{x, \varepsilon} - \theta(F) \rvert = \infty \bigr\}	
	.\] 
\end{definition}

\subsection{Достаточность}
\begin{definition}[]
	Статистика $T(x) = \{T_1(x), \ldots , T_m(x))\}$ называется \selectedFont{достаточной}, если для всех 
	\begin{itemize}[noitemsep]
		\item $\theta \in \Theta$,
		\item $B \in \PP(\R^{n})$ и 
		\item $t = (t_1, \ldots , t_m)$
	\end{itemize}
	 условная вероятность $\mathbb{P}(X_{[n]} \in B \mid T(X_{[n]}) = t)$ не зависит от $\theta$.

	 То есть информация о $\theta$ в выборке полностью содержится в значении $T(x_{[n]})$.
\end{definition}
\begin{theorem}[факторизации]
	$T(x)$ достаточна, согда существуют функции $g$ и  $h$, что
	\[
	p(X_{[n]} = x_{[n]} \mid \theta) = g(T(x_{[n]}), \theta) h(x_{[n]})
	,\] 
	где $p$ --- вероятность или плотность.
\end{theorem}

\subsection{Полнота}
\begin{definition}[]
	Статистика $T$ называется \selectedFont{полной}, если для любой \it{измеримой} $g$ верно следствие
	\[
	 \forall \theta \in  \Theta\colon\E {g (T(X_{[n]}))} \equiv 0 \quad \implies \quad g(T(X_{[n]})) \overset{\text{п.н.}}{=} 0
	.\] 
\end{definition}


\section{Теоремы Колмогорова-Блэкуэлла-Рао и Лемана-Шеффе}
\begin{theorem}[Колмогорова-Блэкуэлла-Рао]
    Пусть $\theta^*$ --- оценка параметра $\theta$, $T$ --- достаточная статистика. Тогда \[
    \MSE(\theta^*) \ge \MSE(\E(\theta^* \mid T))
    .\] 
\end{theorem}
\begin{theorem}[Лемана-Шеффе]
    Пусть $\theta^*$ --- оценка параметра $\theta$, $T$ --- достаточная и полная статистика. Тогда $\E(\theta^* \mid T)$ --- единственная эффективная оценка в классе оценок со смещением $b(\theta^*)$.
\end{theorem}

\section{Доверительный интервал}
Пусть есть модель $\PP_{[n]} = \{\P^{\otimes n} \mid \P \in \PP\}$ и $\theta\colon \PP \to \R^{k}$ --- искомая характеристика.

\begin{definition}[]
	\selectedFont{Доверительный интервал} (точный доверительный интервал) с уровнем доверия $\gamma$ --- пара статистик $(\theta^*_L, \theta^*_R)$, такая что для любого $\P \in \PP$  и $X_{[n]} \sim \P^{\otimes n}$
	\[
	\mathbb{P} \left( \theta^*_L(X_{[n]}) \le \theta(\P) \le \theta^*_R(X_{[n]}) \right) = \gamma 
	.\] 

	Интервал называется
	\begin{itemize}
		\item \selectedFont{асимптотическим}, если
	\[
	\mathbb{P} \left( \theta^*_L(X_{[n]}) \le \theta(\P) \le \theta^*_R(X_{[n]}) \right) \xrightarrow{n \to \infty} \gamma 
	.\] 
		\item \selectedFont{центральным}, если
			\[
			\mathbb{P} \left( \theta^*_L(X_{[n]}) > \theta(\P) \right) = \mathbb{P} \left( \theta^*_R(X_{[n]}) < \theta(\P) \right) 
			.\] 
		\item \selectedFont{левым}, если
			\[
			\mathbb{P} \left( \theta^*_L(X_{[n]}) > \theta(\P) \right) = 0
			.\] 
		\item \selectedFont{правым}, если
			\[
			\mathbb{P} \left( \theta^*_R(X_{[n]}) < \theta(\P) \right) = 0
			.\] 
	\end{itemize}
\end{definition}

\section{Бутстреп}
\subsection{Параметрический бутстреп}
Если работаем с параметрической моделью, можем заменить $X=X(\theta)$ не на $X^*$, а на $X(\theta^*)$ и сэмплировать из этого распределения.
\subsection{Непараметрический бутстреп }
\paragraph{Рецепт}
\begin{enumerate}
	\item изготовим $N$ выборок $x^*_{[n], 1}, \ldots , x^*_{[n], N}$ из эмпирического распределения (рандом с возвращением)
	\item вычисляем $\theta^{b}_{i} = \theta^*(x^*_{[n], i}$, получаем бутстреповскую выборку $\theta^{b}_{[N]}$,
	\item по бутстреповской выборке оцениваем, что нужно.
\end{enumerate}
\paragraph{Ограничения}
\begin{itemize}
	\item $\theta^*$ --- plug-in оценка
    \item $\theta^*$ --- достаточно гладкая (обычно дифференцируема)
	\item у  $X$ достаточно много моментов (обычно конечная дисперсия)
	\item нужно генерировать большие выборки
	\item на очень больших данных трудозатратен
	\item на маленьких данных велика неустранимая ошибка
\end{itemize}

\section{Гипотеза, альтернатива...}
Пусть $\PP$ --- модель.
\subsection{Гипотеза и альтернатива}
\begin{definition}[]
	\selectedFont{Гипотеза} --- утверждение вида $H\colon \P_X \in  \PP_0 \subset \PP$.

	Если $\lvert \PP_0 \rvert = 1$, гипотеза называется \selectedFont{простой}, иначе \selectedFont{сложной}.

	\selectedFont{Нулевая гипотеза} --- гипотеза $H_0$, которую мы хотим проверить. Проверка гипотезы --- процесс принятия решения о том, противоречит ли она наблюдаемой выборке данных.

	\selectedFont{Альтернатива} --- гипотеза $H_1$, которая отражает, какие отклонения от нулевой гипотезы нам интересны.
\end{definition}
\subsection{Критерий}
\begin{definition}[]
	\selectedFont{Нерандомизированный критерий} (критерий) --- отображение $\varphi \colon d \to \{\text{принимаем}, \text{отвергаем}\} = \{H_0, H_1\} = \{0, 1\}$.
\end{definition}
Часто критерий устроен так: имеется
\vspace{-1.5em}
\begin{itemize}[noitemsep]
	\item \selectedFont{статистика критерия} $T$ и
	\item \selectedFont{критическое множество} $C$, и
\end{itemize}
\vspace{-1.5em}
\[
\varphi(d) = [T(d) \in C] = [d \in T^{-1}(C)]
.\] 
\begin{definition}[]
	\selectedFont{Рандомизированный критерий} --- отображение $\varphi\colon d \to [0, 1]$. Значение на данных $d$ определяется как реализация случайной величины $D(\varphi(d))$.
\end{definition}
Пусть мы согласны отвергать нулевую гипотезу пр условии, что она верна, но хотим делать это не очень часто. Пусть зафиксирован \selectedFont{уровень значимости}
\[
	\alpha\coloneqq \mathbb{P} (\varphi(D) = 1 \mid H_0) 
,\] 
который обычно является \textit{параметром критерия}, то есть, задавая его, мы определяем \textit{критическое множество} $C_{\alpha}$ такое, что
\[
	\mathbb{P} (T(D) \in C_{\alpha} \mid H_0) = \alpha
.\] 
Таким образом, для одного критерия определено семейство критических областей $\{C_{\alpha} \mid \alpha \in  [0, 1]\}$, где обычно $C_{\alpha} \subset C_{\alpha'}$, если $\alpha < \alpha'$.

\begin{definition}[]
	\selectedFont{Уровень значимости} --- параметр критерия, который регулирует, насколько часто мы будем отвергать нулевую гипотезу при условии, что она верна.
\end{definition}

\subsection{p-value}
Хотим оценить, насколько гипотеза противоречит наблюдаемым данным.
\begin{definition}[]
	\selectedFont{p-value} --- характеристика противоречия гипотезы наблюдаемым данным:
	\[
	\text{p-value} \coloneqq \arg \min \{\alpha \in [0, 1] \mid T(d) \in C_{\alpha}\}
	.\] 
	Другими словами, p-value --- минимальное значение уровня значимости для данного значения статистики критерия, при котором $H_0$ может быть отвергнута.

	Чем меньше p-value, тем больше гипотеза противоречит данным.
\end{definition}
\subsection{Ошибки разных родов}
\begin{definition}[]
	\selectedFont{Ошибка первого рода} --- событие $\varphi(D) = 1 \mid H_{0}$.
	Если уровень значимости совпадает с вероятностью ошибки первого роба, То критерий называется \selectedFont{точным}.
\end{definition}
Уровень значимости --- вероятность ошибки первого рода.

\begin{definition}[]
	\selectedFont{Ошибка второго рода} $\beta$ --- событие $\varphi(D) = 0 \mid H_{1}$, не отклонили нулевую гипотезу при условии, что была верна альтернатива.

	\selectedFont{Мощность} критерия --- вероятность $1 - \beta$ отклонить $H_0$ при условии, что верна $H_1$.
\end{definition}
Для заданного уровня значимости мы хотим иметь как можно более мощный критерий.
\subsection{Свойства критериев}
\begin{definition}[]
	\selectedFont{Несмещенность} --- мощность всегда не меньше ошибки первого рода, критерий не отдает предпочтение альтернативе. $1 - \beta \ge \alpha$ для всех простых гипотез из $\PP_0$ и простых альтернатив из $\PP_1$.
\end{definition}
\begin{definition}[]
	\selectedFont{Состоятельность} --- $\beta \xrightarrow{n \to \infty} 0$ для всех простых альтернатив из $\PP_1$.
\end{definition}
\begin{definition}[]
	\selectedFont{Асимптотичность} --- $\alpha \xrightarrow{n \to \infty}$ для всех простых гипотез из $\PP_0$.
\end{definition}
\begin{definition}[]
	\selectedFont{Наиболее мощный критерий} для данного уровня значимости $\alpha_0$ и простой альтернативы --- такой критерий $\varphi_1$, что для любого критерия $\varphi_2$ такого, что $\alpha(\varphi_2) \le \alpha_0$:
	\[
	\beta(\varphi_1) \le \beta(\varphi_2)
	.\] 
\end{definition}
\subsection{Размер эффекта}
Во многих случаях важна не только информация о p-value, но и величина наблюдаемого эффекта. Размеры эффекта бывают разные, использование того или иного размера эффекта зависит от контекста.

Вместо сравнения p-value с уровнем значимости для принятия статистического решения можно считать размер эффекта, сравнивать с минимальным практически интересным.

\section{Постановка гипотезы согласия. Критерии Колмогорова и Андерсона-Дарлинга}
\subsection{Постановка гипотезы согласия}
\begin{definition}[]
	\selectedFont{Гипотеза согласия} --- гипотеза о соответствии эмпирического распределения теоретическому распределению вероятностей. 
\end{definition}
Критерии для гипотез согласия бывают
\begin{itemize}
	\item общие --- применимые к любому предполагаемому распределению выборки,
	\item специальные --- применимые к гипотезам, формулирующие согласие с определенным свойством распределений;
\end{itemize}
\begin{itemize}
	\item для простых гипотез,
	\item для сложных гипотез.
\end{itemize}
\subsection{Критерий Колмогорова}
Сравнивает эмпирическое и истинное распределение. Для простой гипотезы.

Пусть $F_0$ непрерывна на $\R$. Определим статистику Колмогорова:
\[
	D_n(x_{[n]}) = \underset{x \in \R}{\sup} \lvert F_{n}^* - F_0(x) \rvert
.\] 
\begin{itemize}
	\item Если $H_0$ верна, то $D_n \left( X_[n] \right) \xrightarrow{\text{п.н.}} 0$;
	\item Если $H_0$ неверна, то $D_{n} \left( X_{[n]} \right) \xrightarrow{\text{п.н.}} \underset{x \in \R}{\sup} \lvert F_X(x) - F_0(x) \rvert > 0$.
\end{itemize}

\subsection{Критерий Андерсона-Дарлинга}
Для простой гипотезы.

Определим статистику критерия Андерсона-Дарлинга:
\begin{align*}
	A^2 &= n \int_{\R}^{} \frac{ \left( F_n^*(x) - F_0(x) \right) ^2}{F_0(x) \left( 1 - F_0(x) \right) } d F_0(x) = \\
		&= -n - \sum_{i=1}^{n} \frac{2i-1}{n} \left[ \ln F_0(X_{(i)} \mid \theta) + \ln \left( 1 - F_0(X_{(n+1-i)} \mid \theta \right)  \right] 
\end{align*}
Статистика $A^2$ при выполнении $H_0$ и непрерывности $F_0$ подчиняется табличному распределению. $C_{\alpha} = (a^2_{1-\alpha}, \infty)$.

\section{Постановка гипотезы о параметрах, проверка через доверительные интервалы, z-test, t-test, бутстреп из нулевой гипотезы}
\subsection{Постановка гипотезы о параметрах}
Пусть $\theta$ --- параметр ($X \sim F(x, \theta)$ ) или характеристика ($\theta = \varphi(F_x)$ ) распределения.

Нулевая гипотеза: $H_0 \colon \theta = \theta_0$.
Типичные альтернативы:
\begin{itemize}
	\item $H_1\colon \theta = \theta_1 \neq \theta_0$,
	\item $H_{>}\colon \theta > \theta_0$,
	\item $H_{<}\colon \theta < \theta_0$,
	\item $H_{\neq}\colon \theta \neq \theta_0$.
\end{itemize}

Усредненный рецепт:
\begin{enumerate}
	\item Выбираем оценку $\theta^*$ параметра $\theta$, распределение которой приближенно известно при данном $\theta$.
	\item В зависимости от альтернативы строим критическое множество:
		\begin{itemize}
			\item $H_1\colon \theta = \theta_1 > \theta_0$ или $H_{>}$, то $C_{\alpha} = (\theta^*_{1-\alpha}, \infty)$ \footnote{Здесь $\theta^*_{x}$ --- квантиль уровня $x$ распределения $\theta^* \mid H_0$} --- правое критическое множество;
			\item $H_1\colon \theta = \theta_1 < \theta_0$ или $H_{<}$, то $C_{\alpha} = (-\infty, \theta^*_{\alpha})$ --- левое критическое множество;
			\item $H_{\neq}$, то $C_{\alpha} = (-\infty, \theta^*_{\frac{\alpha}{2}} \cup (\theta^*_{1 - \frac{\alpha}{2}}, \infty)$ --- двустороннее критическое множество.
		\end{itemize}
	\item Если $\theta^* \in  C_{\alpha}$, то гипотезу можно отклонить, иначе --- нельзя.
\end{enumerate}

\subsection{Проверка через доверительные интервалы}
Усредненный рецепт:
\begin{enumerate}
	\item В зависимости от альтернативы строим доверительный интервал с уровнем доверия $\gamma = 1 - \alpha$:
		\begin{itemize}
			\item $H_1\colon \theta = \theta_1 > \theta_0$ или $H_{>}$, то $(\theta^*_{L}, \infty)$ --- правый доверительный интервал;
			\item $H_1\colon \theta = \theta_1 < \theta_0$ или $H_{<}$, то $(-\infty, \theta^*_{R})$ --- левый доверительный интервал;
			\item $H_{\neq}$, то $(\theta^*_{L} , \theta^*_{R})$ --- центральный доверительный интервал.
		\end{itemize}
\end{enumerate}

\subsection{z-test}
Пусть $X \sim \N(\mu, \sigma^2)$, $\mu$ неизвестно, $\sigma^2$ известно.

Если $H_0$ верна, то $Z = \frac{\sqrt{n} (\overline{X} - \mu_0)}{\sigma} \sim \N(0, 1)$ и $\overline{X} \sim N(\mu_0, \frac{\sigma^2}{n})$.

В зависимости от альтернативы подбираем критическую область:
\begin{table}[htpb]
	\centering
	\label{tab:crytical_space_z_test}

	\begin{tabular}{cccc}
		 & $\theta_1 > \theta_0$, $H_{>}$ & $\theta_1 < \theta_0$, $H_{<}$  & $H_{\neq}$ 
		\\
		\hline
		$C_{\alpha}$ & $(\mu_0 + z_{1-\alpha} \frac{\sigma}{\sqrt{n}}, \infty)$ &$(- \infty, \mu_0 + z_{\alpha} \frac{\sigma}{\sqrt{n}})$ & $\R \setminus (\mu_0 \pm z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}})$
		\\
		\hline
		p-value & $1 - \Phi^{-1}(z)$ & $\Phi^{-1}(z)$ & $2(1-\Phi^{-1}(\lvert z \rvert))$
	\end{tabular}
	\caption{Критическая область для альтернативы}
\end{table}
\subsection{t-test}
Пусть $X \sim \N(\mu, \sigma^2)$, $\mu$ неизвестно, $\sigma^2$ неизвестно.

Если $H_0$ верна, то $T = \frac{\sqrt{n} (\overline{X} - \mu_0)}{s} \sim T(n-1)$.

В зависимости от альтернативы подбираем критическую область:
\begin{table}[htpb]
	\centering
	\label{tab:crytical_space_t_test}

	\begin{tabular}{cccc}
		 & $\theta_1 > \theta_0$, $H_{>}$ & $\theta_1 < \theta_0$, $H_{<}$  & $H_{\neq}$ 
		\\
		\hline
		$C_{\alpha}$ & $(\mu_0 + t_{1-\alpha} \frac{s}{\sqrt{n}}, \infty)$ &$(- \infty, \mu_0 + t_{\alpha} \frac{s}{\sqrt{n}})$ & $\R \setminus (\mu_0 \pm t_{1-\frac{\alpha}{2}} \frac{s}{\sqrt{n}})$
		\\
		\hline
		p-value & $1 - T^{-1}(t)$ & $T^{-1}(t)$ & $2(1-T^{-1}(\lvert t \rvert))$
	\end{tabular}
	\caption{Критическая область для альтернативы}
\end{table}

\subsection{Бутстреп из нулевой гипотезы}
Пусть мы хотим проверить гипотезу $H_0\colon \E X = \theta_0$.

Рецепт:
\begin{enumerate}
	\item Назначим каждому наблюдению $x_i$ в выборке вероятность $p_i$.
	\item Из пар $(x_i, p_i)$ изготовим дискретное распределение $F^*_p$.
	\item Подберем $p_i$ так, чтобы с одной стороны $\overline{x} = \theta_0$, а с другой $p_i$ максимизировали правдоподобие выборки $\mathcal{L}(p \mid x_{[n]}) = p_1 p_2 \ldots p_{n}$.
	\item Бутстрепим кучу выборок из получившегося $F_{p}^{*}$, считаем по ним выборочное среднее.
	\item Построим критическое множество в зависимости от альтернативы и проверим, лежит ли в нем выборочное среднее исходной выборки.
\end{enumerate}
\end{document}
